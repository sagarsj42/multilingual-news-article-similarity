{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /scratch/sagarsj42/torch-cache\n",
    "!mkdir -p /scratch/sagarsj42/transformers\n",
    "import os\n",
    "os.chdir('/scratch/sagarsj42')\n",
    "os.environ['TORCH_HOME'] = '/scratch/sagarsj42/torch-cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/sagarsj42/transformers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-2022-task-8-train-df.csv              100%   27MB  27.3MB/s   00:00    \n",
      "semeval-2022-task-8-eval-df.csv               100%   27MB  27.4MB/s   00:01    \n"
     ]
    }
   ],
   "source": [
    "!scp sagarsj42@ada:/share1/sagarsj42/semeval-2022-task-8-train-df.csv .\n",
    "!scp sagarsj42@ada:/share1/sagarsj42/semeval-2022-task-8-eval-df.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4467 entries, 3913 to 735\n",
      "Data columns (total 20 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   pair_id             4467 non-null   object \n",
      " 1   score               4467 non-null   float64\n",
      " 2   id_1                4467 non-null   int64  \n",
      " 3   title_1             4427 non-null   object \n",
      " 4   text_1              4436 non-null   object \n",
      " 5   keywords_1          4467 non-null   object \n",
      " 6   meta_keywords_1     4467 non-null   object \n",
      " 7   tags_1              4467 non-null   object \n",
      " 8   summary_1           11 non-null     object \n",
      " 9   meta_description_1  3720 non-null   object \n",
      " 10  meta_lang_1         3869 non-null   object \n",
      " 11  id_2                4467 non-null   int64  \n",
      " 12  title_2             4433 non-null   object \n",
      " 13  text_2              4426 non-null   object \n",
      " 14  keywords_2          4467 non-null   object \n",
      " 15  meta_keywords_2     4467 non-null   object \n",
      " 16  tags_2              4467 non-null   object \n",
      " 17  summary_2           10 non-null     object \n",
      " 18  meta_description_2  3575 non-null   object \n",
      " 19  meta_lang_2         3887 non-null   object \n",
      "dtypes: float64(1), int64(2), object(17)\n",
      "memory usage: 732.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('semeval8-train-sss.csv', index_col=0)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>score</th>\n",
       "      <th>id_1</th>\n",
       "      <th>title_1</th>\n",
       "      <th>text_1</th>\n",
       "      <th>keywords_1</th>\n",
       "      <th>meta_keywords_1</th>\n",
       "      <th>tags_1</th>\n",
       "      <th>summary_1</th>\n",
       "      <th>meta_description_1</th>\n",
       "      <th>meta_lang_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>title_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>keywords_2</th>\n",
       "      <th>meta_keywords_2</th>\n",
       "      <th>tags_2</th>\n",
       "      <th>summary_2</th>\n",
       "      <th>meta_description_2</th>\n",
       "      <th>meta_lang_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>1593242729_1506953769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1593242729</td>\n",
       "      <td>Brände: Stall mit 1000 Schweinen brennt ab</td>\n",
       "      <td>Schöppingen. Ein Schweinestall mit rund 1000 T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Brände', 'Tiere', 'NRW', 'Nordrhein-Westfale...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ein Schweinestall mit rund 1000 Tieren ist am ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1506953769</td>\n",
       "      <td>Neue Flüchtlingsunterkunft des Landes in Dorsten</td>\n",
       "      <td>In die neue Landesunterkunft sollen vor allem ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>1537050788_1536175708</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1537050788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Przykro nam, ale z adresu IP z którego się łąc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pl</td>\n",
       "      <td>1536175708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Przykro nam, ale z adresu IP z którego się łąc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>1544221535_1544175685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1544221535</td>\n",
       "      <td>Diese Raupe isst Plastik und pinkelt Alkohol</td>\n",
       "      <td>Die Raupen der Grossen Wachsmotte verbringen i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['20minuten.ch', 'www.20minuten.ch', '20 Minut...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Die Raupe der Grossen Wachsmotte ist in der La...</td>\n",
       "      <td>de</td>\n",
       "      <td>1544175685</td>\n",
       "      <td>Winziger plastikessender Wurm unter wissenscha...</td>\n",
       "      <td>Die Fähigkeit der Würmer, sich von Polyethylen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Winziger Wachswurm', 'Raupe', 'besondere Bak...</td>\n",
       "      <td>['Bakterien', 'Plastik', 'fressen', 'Würmer', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Forscher von der Brandon University in Kana...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>1592201688_1617897198</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1592201688</td>\n",
       "      <td>Caddeyi saksafon sesiyle şenlendirdi</td>\n",
       "      <td>Caddeyi saksafon sesiyle şenlendirdi\\n\\nKırkla...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['İstanbul Caddesi', 'Kırklareli', 'Lüleburgaz...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caddeyi saksafon sesiyle şenlendirdi Kırklarel...</td>\n",
       "      <td>tr</td>\n",
       "      <td>1617897198</td>\n",
       "      <td>Kısıtlama sonrası korkutan rehavet görüntüsü</td>\n",
       "      <td>Kısıtlama sonrası korkutan rehavet görüntüsü K...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['İstanbul Caddesi', 'Kırklareli', 'Lüleburgaz...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kısıtlama sonrası korkutan rehavet görüntüsü -...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1484037661_1485647004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1484037661</td>\n",
       "      <td>Giuliani Says He’s Prepared to ‘Do Demonstrati...</td>\n",
       "      <td>Rudy Giuliani is prepared to do more than just...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Donald J. Trump', 'Ukraine', 'Rudy Giuliani'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He’s also prepared to “try the case” and “give...</td>\n",
       "      <td>en</td>\n",
       "      <td>1485647004</td>\n",
       "      <td>VERY interesting comment from Giuliani (raw)…</td>\n",
       "      <td>What the hell just happened? pic.twitter.com/Q...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>America's Front Page For Political News</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pair_id  score        id_1  \\\n",
       "3913  1593242729_1506953769    4.0  1593242729   \n",
       "2962  1537050788_1536175708    2.0  1537050788   \n",
       "2605  1544221535_1544175685    1.0  1544221535   \n",
       "3075  1592201688_1617897198    3.0  1592201688   \n",
       "501   1484037661_1485647004    1.0  1484037661   \n",
       "\n",
       "                                                title_1  \\\n",
       "3913         Brände: Stall mit 1000 Schweinen brennt ab   \n",
       "2962                                                NaN   \n",
       "2605       Diese Raupe isst Plastik und pinkelt Alkohol   \n",
       "3075               Caddeyi saksafon sesiyle şenlendirdi   \n",
       "501   Giuliani Says He’s Prepared to ‘Do Demonstrati...   \n",
       "\n",
       "                                                 text_1 keywords_1  \\\n",
       "3913  Schöppingen. Ein Schweinestall mit rund 1000 T...         []   \n",
       "2962  Przykro nam, ale z adresu IP z którego się łąc...         []   \n",
       "2605  Die Raupen der Grossen Wachsmotte verbringen i...         []   \n",
       "3075  Caddeyi saksafon sesiyle şenlendirdi\\n\\nKırkla...         []   \n",
       "501   Rudy Giuliani is prepared to do more than just...         []   \n",
       "\n",
       "                                        meta_keywords_1 tags_1 summary_1  \\\n",
       "3913  ['Brände', 'Tiere', 'NRW', 'Nordrhein-Westfale...     []       NaN   \n",
       "2962                                               ['']     []       NaN   \n",
       "2605  ['20minuten.ch', 'www.20minuten.ch', '20 Minut...     []       NaN   \n",
       "3075  ['İstanbul Caddesi', 'Kırklareli', 'Lüleburgaz...     []       NaN   \n",
       "501   ['Donald J. Trump', 'Ukraine', 'Rudy Giuliani'...     []       NaN   \n",
       "\n",
       "                                     meta_description_1 meta_lang_1  \\\n",
       "3913  Ein Schweinestall mit rund 1000 Tieren ist am ...         NaN   \n",
       "2962                                                NaN          pl   \n",
       "2605  Die Raupe der Grossen Wachsmotte ist in der La...          de   \n",
       "3075  Caddeyi saksafon sesiyle şenlendirdi Kırklarel...          tr   \n",
       "501   He’s also prepared to “try the case” and “give...          en   \n",
       "\n",
       "            id_2                                            title_2  \\\n",
       "3913  1506953769   Neue Flüchtlingsunterkunft des Landes in Dorsten   \n",
       "2962  1536175708                                                NaN   \n",
       "2605  1544175685  Winziger plastikessender Wurm unter wissenscha...   \n",
       "3075  1617897198       Kısıtlama sonrası korkutan rehavet görüntüsü   \n",
       "501   1485647004      VERY interesting comment from Giuliani (raw)…   \n",
       "\n",
       "                                                 text_2 keywords_2  \\\n",
       "3913  In die neue Landesunterkunft sollen vor allem ...         []   \n",
       "2962  Przykro nam, ale z adresu IP z którego się łąc...         []   \n",
       "2605  Die Fähigkeit der Würmer, sich von Polyethylen...         []   \n",
       "3075  Kısıtlama sonrası korkutan rehavet görüntüsü K...         []   \n",
       "501   What the hell just happened? pic.twitter.com/Q...         []   \n",
       "\n",
       "                                        meta_keywords_2  \\\n",
       "3913                                               ['']   \n",
       "2962                                               ['']   \n",
       "2605  ['Winziger Wachswurm', 'Raupe', 'besondere Bak...   \n",
       "3075  ['İstanbul Caddesi', 'Kırklareli', 'Lüleburgaz...   \n",
       "501                                                ['']   \n",
       "\n",
       "                                                 tags_2 summary_2  \\\n",
       "3913                                                 []       NaN   \n",
       "2962                                                 []       NaN   \n",
       "2605  ['Bakterien', 'Plastik', 'fressen', 'Würmer', ...       NaN   \n",
       "3075                                                 []       NaN   \n",
       "501                                                  []       NaN   \n",
       "\n",
       "                                     meta_description_2 meta_lang_2  \n",
       "3913                                                NaN          de  \n",
       "2962                                                NaN          pl  \n",
       "2605  <p>Forscher von der Brandon University in Kana...          de  \n",
       "3075  Kısıtlama sonrası korkutan rehavet görüntüsü -...          tr  \n",
       "501             America's Front Page For Political News          en  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "encoder = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pair_id', 'score', 'id_1', 'title_1', 'text_1', 'keywords_1',\n",
       "       'meta_keywords_1', 'tags_1', 'summary_1', 'meta_description_1',\n",
       "       'meta_lang_1', 'id_2', 'title_2', 'text_2', 'keywords_2',\n",
       "       'meta_keywords_2', 'tags_2', 'summary_2', 'meta_description_2',\n",
       "       'meta_lang_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_df.iloc[0]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair_id': '1593242729_1506953769',\n",
       " 'text_1': 'schöppingen. ein schweinestall mit rund 1000 tieren ist am mittwochmorgen in schöppingen im münsterland abgebrannt.\\n\\netwa 900 schweine verendeten in den flammen, 100 befanden sich am nachmittag noch mit schweren verbrennungen in dem einsturzgefährdeten gebäude, wie ein sprecher des kreises borken sagte. die überlebenden tiere müssten vermutlich durch das kreisveterinäramt getötet werden, da sie aufgrund der verbrennungen und rauchvergiftungen „keine hohen überlebenschancen haben“, erklärte ein polizeisprecher.\\n\\nein 47 jahre alter landwirt hatte nach polizeiangaben noch versucht, die flammen zu löschen. er habe durch die starke rauchentwicklung selbst eine rauchvergiftung erlitten und sei ins krankenhaus gekommen. die polizei vermutet nach ersten ermittlungen einen technischen defekt als brandursache. der schaden beläuft sich demnach auf etwa 900 000 euro. angrenzende gebäude wurden nicht beschädigt.',\n",
       " 'text_2': 'in die neue landesunterkunft sollen vor allem familien einziehen. dort sollen sie so lange bleiben, bis über ihren asylantrag entschieden ist. spätestens ein halbes jahr nach ankunft der menschen wollen die behörden entscheiden, ob sie einer stadt oder gemeinde zugewiesen werden.\\n\\nfünfte zue im münsterland\\n\\nmehr als ein jahr hat es gedauert, die ehemalige schule in dorsten dafür umzubauen. die kosten liegen bei rund zweieinhalb millionen euro. in dorsten geht damit nun die fünfte so genannte zentrale unterbringungseinrichtung im münsterland in betrieb. weitere gibt es derzeit in münster, ibbenbüren, rheine und schöppingen, wo der vertrag noch bis ende kommenden jahres läuft.',\n",
       " 'score': 3}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict = train_df.iloc[0][['pair_id', 'text_1', 'text_2', 'score']].to_dict()\n",
    "sample_dict['text_1'] = sample_dict['text_1'].lower().strip()\n",
    "sample_dict['text_2'] = sample_dict['text_2'].lower().strip()\n",
    "sample_dict['score'] = round(sample_dict['score']) - 1\n",
    "sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 222]), torch.Size([1, 222]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sample['text_1']\n",
    "text_tok = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "text_tok['input_ids'].shape, text_tok['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 222, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_enc = encoder(**text_tok)\n",
    "text_enc.last_hidden_state.shape, text_enc.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityClassifier(nn.Module):\n",
    "    def __init__(self, encoder, embed_size=768, hidden_size=512, n_classes=4):\n",
    "        super(SimilarityClassifier, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.embed_size, self.hidden_size)\n",
    "        self.activation1 = nn.LeakyReLU(negative_slope=0.1)\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.n_classes)\n",
    "        self.activation2 = nn.Softmax(dim=-1)\n",
    "\n",
    "    def common_compute(self, x):\n",
    "        x = self.encoder(**x).pooler_output\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.common_compute(x1)\n",
    "        x2 = self.common_compute(x2)\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimilarityClassifier(\n",
       "  (encoder): XLMRobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (activation1): LeakyReLU(negative_slope=0.1)\n",
       "  (linear2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (activation2): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimilarityClassifier(encoder, embed_size=768, hidden_size=512, n_classes=4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualNewsSimDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super(MultilingualNewsSimDataset, self).__init__()\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx][['pair_id', 'text_1', 'text_2', 'score']].to_dict()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4467,\n",
       " {'pair_id': '1558576281_1558746898',\n",
       "  'text_1': \"İşsizlik maaşı şartları nelerdir? Soruları işsizlik maaşı alacak olan işçi vatandaşlar tarafından sorgulanıyor. İş yerinden ayrılmış, sözleşmesi feshedilmiş çalışanlar İŞKUR üzerinden başvuru formu doldurarak işsizlik maaşını almaya hak kazanıyorlar. Peki, işsizlik maaşı başvuru nasıl yapılır?\\n\\nİŞSİZLİK MAAŞI ŞARTLARI NELERDİR?\\n\\nİşsizlik maaşı alabilmek için, son 3 yılda en az 600 gün prim ödenmesi ve son 120 gün hizmet akdiyle çalışılması gerekiyor. İŞKUR'a işsizlik başvurusunun, iş akdinin feshinden itibaren 30 gün içinde yapılması gerekiyor.\\n\\n\\n\\nİŞSİZLİK BAŞVURU VE MAAŞI SORGULAMA\\n\\nİşsizlik maaşı ile ilgili sorgulamalarınız için E Devlet Kapısı'nı kullanabilirsiniz.\\n\\n(Başvuru için: https://www.turkiye.gov.tr/issizlik-odenegi-basvurusu)\\n\\n(İşsizlik ödeneği sorgulamak için: https://www.turkiye.gov.tr/issizlik-odenegi-odemesi)\\n\\nNE KADAR SÜRE İŞSİZLİK MAAŞI ALINABİLİR?\\n\\nİŞKUR'un resmi internet sitesinde yaptığı bilgilendirmeye göre; İşsizlik ödeneğinin süresi\\n\\n- 600 gün sigortalı olarak çalışıp işsizlik sigortası primi ödemiş olan sigortalı işsizlere 180 gün,\\n\\n- 900 gün sigortalı olarak çalışıp işsizlik sigortası primi ödemiş olan sigortalı işsizlere 240 gün,\\n\\n- 1080 gün sigortalı olarak çalışıp işsizlik sigortası primi ödemiş olan sigortalı işsizlere 300 gün.\\n\\nİŞSİZLİK MAAŞI NE KADAR 2020?\\n\\nAsgari ücret zammının açıklanmasının ardından 2020 yılındaki işsizlik maaşı da belli oldu. Buna göre brüt asgari ücret üzerinden hesaplanan işsizlik maaşı 2020’de en düşük bin 177 lira, en yüksek 2 bin 354 lira olarak uygulanacak.\\n\\nAile, Çalışma ve Sosyal Hizmetler Bakanı Selçuk, asgari ücretin 1 Ocak 2020 tarihinden itibaren brüt 2 bin 943 lira, net 2 bin 324 lira 70 kuruş olacağını açıkladı. Bu açıklamayla işsizlik maaşları da belli oldu. Bu maaşın sınırları geçerli asgari ücrete göre belirleniyor ve en düşüğü brüt asgari ücretin yüzde 40'ı, en yükseği ise yüzde 80’i oranında oluyor. Bu ücretten sadece damga vergisi kesiliyor. Bu detaylar ışığında damga vergisi kesilmeden önce işsizlik maaşı 2020 yılında en düşük bin 177 lira, en yüksek 2 bin 354 lira olarak uygulanacak.\",\n",
       "  'text_2': 'Sağlık Bakanı Fahrettin Koca ve Eğitim Bakanı Ziya Selçuk, Bilim Kurulu toplantısı sonrası ortak açıklama yaptı. Eğitim Bakanı Ziya Selçuk uzaktan eğitimin 30 Nisan\\'a kadar uzatıldığını açıkladı. Sağlık Bakanı Koca\\'nın açıklamasına göre ise Türkiye genelinde vaka sayısı 1872, yoğun bakım hasta sayısı 136, entübe hasta sayısı 102, iyileşen kişi sayısı 26 oldu.\\n\\n\"İKİ HASTAMIZ TABURCU OLDU\"\\n\\nFahrettin Koca\\'nın açıklamasından satır başları:\\n\\n\"Hastalığa karşı elimizde bir koz var; yakalanmamak. Bu hastalığa karşı irade sahibiyiz”\\n\\n\"Koronavirüse karşı verdiğimiz mücadelenin başarısı tek tek bireylere bağlıdır. Devlet bu mücadelede yaptırım gücü olan bir kılavuzdur.\"\\n\\n\"Dikkatli olmalısınız, sosyal hayatınızı asgari düzeye indirmelisiniz\"\\n\\n\"Orta yaşlardaki vaka sayısı az değil. Virüs genç, yaşlı, orta yaşlı ayrımı yapmıyor.\"\\n\\n\"Okullarımız koronavirüs sebebiyle tatile girdi şeklinde bir kanı var. Bu yanlıştır. Çocuklarımızın eğitimi devam ediyor\"\\n\\n\"Okullara verilen ara uzatılacak\"\\n\\n\"Yaşları 60 ve 65 olan iki hastamız taburcu oldu. Erken dönemde müdahalenin yapılması önemli\"\\n\\n\"İnsanların kendi evine, odasına çekilerek kendini izole etmesini, olağanüstü halini kendince uygulamasını son derece önemsiyoruz\"\\n\\n\"Sağlık Bakanlığı Türkiye Geneli Koronavirüs Tablosu\\'na göre vaka sayısı 1872, yoğun bakım hasta sayısı 136, entübe hasta sayısı 102, iyileşen kişi 26.\"\\n\\n\"83 milyonun test yaptırması gerekmiyor, dünyada herkesin test yaptırması diye bir uygulama yok.\"\\n\\n\"Herkes, bu virüsü taşıyor olduğunu, bunu birilerine bulaştırabileceğini düşünerek davranmalı.\"\\n\\n\"Hiçbir test ücret karşılığı yapılsın istemiyoruz. Ne bakanlık ne üniversiteler herhangi bir ücret almak durumunda değiller.\"\\n\\n\"(Çin\\'den yardım talebiniz oldu mu?) \"Büyükelçi ile görüştüğümüzde Bilim Kurulu ile bilim insanlarının karşılıklı tecrübelerini paylaşmaları konusunu konuştuk. Önümüzdeki günlerde planlanacaktır. Başka bir talebimiz olmadı. Çin her açıdan destek olmak istediklerini belirtti\"\\n\\n\"(Sahibi olduğu Medipol Hastanesinde çalışanların zorunlu ücretsiz izne çıkartıldığı sorusu) Ben ilişkiyi kestim. Oradaki çalışanlara sorun beni son 1 senedir görmüşler mi? Böyle bir süreçte bu mümkün değil. Lütfen bunlara inanmayalım\"\\n\\n\"(Sağlık çalışanlarının koruyucu ekipmanlara ulaşmasındaki sorun) Çalışanlarımızı malzemesiz bırakmayacağımıza inanın. N95 maskeyi herkes kullanmamalı\"\\n\\n\"(Çalışmak zorunda olanlar var. İşverenlere söylemek istedikleriniz olur mu?) Kamuda düzenleme yaptık, özel sektörden de bunu bekliyoruz\"\\n\\nEVDE EĞİTİM SÜRECİ 30 NİSAN\\'A UZATILDI\\n\\nEğitim Bakanı Ziya Selçuk\\'un açıklamasından satır başları:\\n\\n\"MEB olarak meselye pedogojik olarak bakıyoruz. Çocuklarımızın fiziksel ve ruhsal sağlığına bakıyoruz. İhtiyaçları diğer ülkeleri takip ederek tespit etmeye çalışıyoruz.\"\\n\\n\"Uzaktan eğitime başladık bu hafta test haftasıydı. Önümüzdeki hafta çok daha kaliteli uzaktan eğitim yapmaya devam edeceğiz\"\\n\\n\"Evde eğitim süreci 30 Nisan tarihine kadar uzatıldı\"\\n\\n\"Ücretli öğretmenler konusundaki mevzuat çok açık. Bizim telafi eğitim olarak planladığımız zaman dilimlerinde derslere ücretli öğretmenlerin girmesi planlanıyor.\"\\n\\n\"(Üniversite sınavları ertelenecek mi?) Sınavlar konusu işin başından beri gündemimizde. Sınavın zamanında yapılmasını önemseriz. LGS için 3 senaryo var. Sınavların ertelenmesi veya ertelenmemesi bilimsel metadoloji sonucunda ortaya çıkacak kararlara bağlı. YKS\\'nin kararını ise Yükseköğretim Kurulu ve ÖSYM verecek. İstişaremiz devam ediyor\"\\n\\n\"(Özel okul ücretleri) Özel okul temsilcileriyle görüşmemizde gündeme geldi, telafi edileceği için problem çıkacağını sanmıyorum.\" (HABER MERKEZİ)\\n\\nTürkiye\\'de koronavirüsten 59 can kaybı | Tüm gelişmeler (25 Mart)',\n",
       "  'score': 4.0})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MultilingualNewsSimDataset(train_df)\n",
    "len(train_dataset), train_dataset[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497,\n",
       " {'pair_id': '1484038753_1484148859',\n",
       "  'text_1': 'California is poised to lose a congressional seat for the first time in its history as a state, based on U.S. Census Bureau population estimates released Monday that showed the nation’s growth continued to slow in 2019.\\n\\nSome 27 states and the District of Columbia lost residents through net domestic migration between 2018 and 2019, the new census data show.\\n\\nAbout 203,000 people left California in that period, a result of the state’s shifting migration patterns and economic strains that are making it harder to afford living here. New York, Illinois, New Jersey, Massachusetts and Louisiana also saw large losses to other states.\\n\\nCalifornia’s potential loss in reapportionment, which will be determined by next year’s census count, would drop the state’s number of seats in the U.S. House of Representatives from 53 to 52, said William Frey, a senior fellow at the Brookings Institution.\\n\\nAdvertisement\\n\\n“It’s got a lot to do with dispersion from California to the rest of the west,” Frey said. “Arizona, Texas and Colorado are all big destinations for California migrants, and they all are gaining seats.”\\n\\nA 2019 relocation study by Texas Realtors found that 63,175 Californians moved to Texas in 2017, while California was the top destination for Texans to move — nearly 41,000 relocated here.\\n\\nTexas is likely to gain three seats following the 2020 decennial count, according to Frey’s analysis of census data, while states such as Arizona, Colorado and Oregon may gain one seat apiece.\\n\\nThe apportionment population count for each of the 50 states includes the state’s total resident population — citizens and non-citizens — as well as a count of the overseas federal employees and their dependents who have that state listed as their home state in their employers’ administrative records, according to the U.S. Census Bureau.\\n\\nAdvertisement\\n\\nThe House of Representatives is limited to 435 members, not under the U.S. Constitution, but because of a 1929 federal law that could be changed if lawmakers and the president agreed to do so.\\n\\nExactly where California would lose a seat in the House depends on which communities are larger or smaller compared to census numbers from 2010. The state’s Citizens Redistricting Commission, whose members will be selected in coming months, will hold public hearings in 2021 to determine how to redraw congressional maps.\\n\\nPaul Mitchell, one of the state’s leading analysts of the redistricting process, said that two places could dominate the discussion: the communities sitting at the intersection of Los Angeles, Orange and San Bernardino counties and the suburbs to the east of San Francisco.\\n\\nBut other big changes to the political map-drawing process are also in store — including the 2013 ruling by the U.S. Supreme Court to strike part of the federal Voting Rights Act that strongly influenced the current California maps.\\n\\n“That will allow a massive rewrite of the Central Valley congressional districts, so it might be really hard to see the total impact” of losing a House seat because of population, Mitchell said.\\n\\nEven so, the most obvious political impact would be to force incumbent House members to either run against each other or leave office. In 2012, Rep. Brad Sherman (D-Northridge) defeated former Rep. Howard Berman in a bitter contest brought on by the new lines drawn in Los Angeles County.\\n\\nCalifornia’s future numerical strength in Congress hinges in part on making sure that members of historically undercounted groups are included in the census count. In California, 72% of the population belongs to one of these groups, according to the Public Policy Institute of California.\\n\\nState census workers, community organizations and local politicians started outreach efforts as early as April to ensure an accurate tally in next year’s count. In addition to reapportionment, nearly $800 billion in federal tax dollars and political redistricting are at stake.\\n\\nAdvertisement\\n\\nState government leaders have allocated about $187 million to help verify addresses and expand outreach efforts, according to California’s census office.\\n\\nStill, there will be major hurdles. Those without reliable internet connections may be missed in a census that will rely heavily on online surveys. Los Angeles County, officials say, will be the nation’s hardest to tally because of its high concentrations of renters and homeless people, as well as immigrant communities that may not participate, either because of language barriers or because they fear being targeted by federal immigration authorities.\\n\\n“If, as many fear, non-citizen populations and the state’s heavily Latino population either fails to participate or participates without providing full household counts, then California could lose more than one seat,” said Mitchell, whose firm analyzes political data for regional and statewide candidates.\\n\\nNationally, natural increase (births minus deaths) has declined steadily over the past decade. The U.S. also registered a decline in its population under the age of 18, Frey, the demographer, said. California led in that category, with a drop of about 400,000 people under the age of 18, followed by Illinois and New York. The South and West saw the biggest gain in children, Frey added, led by Texas.\\n\\n“This is a symptom of an aging population,” he said, “and in states like California, an out-migration of younger families with children.”\\n\\nCalifornia’s representation stayed the same following the 2010 census count. If the state does lose a seat or two in 2020, Frey said, it’s uncertain whether the decline would carry into the 2030 count.\\n\\n“In a way, these last few years are a confluence of things that may not continue over time,” he said. “The slowdown in immigration may not continue, millennials may finally start having kids. Domestic out-migration may continue. All those things happened at the same time, so I don’t think they’re going to be quite as dim in the next few years.”',\n",
       "  'text_2': 'By JOSH FRIEDMAN\\n\\nFueled by residents leaving the state, largely due to the high cost of living, California is currently at risk of losing a congressional seat. [LA Times]\\n\\nOn Monday, the U.S. Census Bureau released population estimates. California was one of 27 states that lost residents through net domestic migration. California lost about 203,000 residents.\\n\\nNext year’s census count will determine congressional reapportionment. If California were to lose a congressional seat, it would mark the first time that occurred in California’s history as a state.\\n\\nCalifornia’s potential loss in reapportionment would decrease the state’s seat total in the House of Representatives from 53 to 52.\\n\\nWilliam Frey, a senior fellow at the Brookings Institution, said the potential loss of a congressional seat has a lot to do with California residents moving to Arizona, Texas, Colorado and other western states.\\n\\nTexas, the state to which an estimated 63,000 Californians moved in 2017, stands to potentially gain three congressional seats. Arizona, Colorado and Oregon may gain one seat each.\\n\\nCalifornia’s Citizens Redistricting Commission will hold public hearings in 2021 to determine how to redraw congressional maps.\\n\\nPaul Mitchell, a leading analyst of the redistricting process, said two areas that may be at risk of losing a congressional seat are the communities located at the intersection of Los Angeles, Orange and San Bernardino counties and the suburbs east of San Francisco.\\n\\nHowever, other changes to congressional maps, particularly in the Central Valley, are also expected, so it might be difficult to notice the impact of losing a House seat due to population, Mitchell said.\\n\\nBut, Mitchell also said if California’s Latino and non-citizen populations do not participate in the census as much as state officials would like, California could be at risk of losing more than one seat.',\n",
       "  'score': 1.0})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = pd.read_csv('semeval8-dev.csv')\n",
    "dev_dataset = MultilingualNewsSimDataset(dev_df)\n",
    "len(dev_dataset), dev_dataset[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, num_classes=4, type='train'):\n",
    "    texts_1, texts_2, scores = list(), list(), list()\n",
    "    for sample in batch:\n",
    "        text1 = str(sample['text_1']).lower().strip()\n",
    "        text2 = str(sample['text_2']).lower().strip()\n",
    "\n",
    "        score_indx = round(sample['score']) - 1\n",
    "        score = torch.zeros((1, num_classes))\n",
    "        score[0, score_indx] = 1\n",
    "        \n",
    "        texts_1.append(text1)\n",
    "        texts_2.append(text2)\n",
    "        scores.append(score)\n",
    "\n",
    "    texts_1 = tokenizer(texts_1, truncation=True, padding=True, return_tensors='pt')\n",
    "    texts_2 = tokenizer(texts_2, truncation=True, padding=True, return_tensors='pt')\n",
    "    scores = torch.cat(scores, dim=0)\n",
    "\n",
    "    return (texts_1, texts_2) if type == 'test' else (texts_1, texts_2, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[     0,  16188,  80059,  ...,  80939,      5,      2],\n",
       "         [     0,    359,  27661,  ...,      1,      1,      1],\n",
       "         [     0,     95,   5800,  ...,      1,      1,      1],\n",
       "         [     0, 121477,   1681,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[     0,    388,  61909,  ..., 149976,     23,      2],\n",
       "         [     0,  27498,  38126,  ...,      1,      1,      1],\n",
       "         [     0,   1642,  15234,  ...,      1,      1,      1],\n",
       "         [     0, 121477,   1681,  ...,  14810, 119477,      2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])},\n",
       " tensor([[0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_partial = partial(collate_fn, tokenizer=tokenizer, num_classes=4, type='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_partial)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2470, 0.2540, 0.2488, 0.2502],\n",
       "        [0.2465, 0.2543, 0.2493, 0.2499],\n",
       "        [0.2470, 0.2541, 0.2486, 0.2503],\n",
       "        [0.2467, 0.2541, 0.2488, 0.2504]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2, scores = batch\n",
    "op = model(x1, x2)\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57ba542a7e01434f0efba8458620b8a5d586d5cdb58a6ff11a95f38e977018f2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
